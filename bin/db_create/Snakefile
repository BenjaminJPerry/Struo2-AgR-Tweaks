#-- Parsing input for db-create workflow --#

# outdir
config['output_dir'] = config['output_dir'].rstrip('/') + '/'
config['db_name'] = config['db_name'].rstrip('/') + '/'

# Samples table
if not os.path.isfile(config['samples_file']):
    raise IOError('Cannot find file: {}'.format(config['samples_file']))
config['samples'] = pd.read_csv(config['samples_file'], sep='\t')
for f in [config['samples_col'], config['fasta_file_path_col'],
          config['taxID_col'], config['taxonomy_col']]:
    if f not in config['samples'].columns:
        raise ValueError('Cannot find column: {}'.format(f))
config['samples'][config['samples_col']] = config['samples'][config['samples_col']].str.replace('[^A-Za-z0-9]+', '_')
config['samples'] = config['samples'].set_index(config['samples'][config['samples_col']])

## check that files exist (skipping if not)
rowID = 0
to_rm = []
for index,row in config['samples'].iterrows():
    rowID += 1
    file_cols = [config['fasta_file_path_col']]
    for f in file_cols:
        if not os.path.isfile(str(row[f])):
           msg = 'Samples table (Row {}): Cannot find file: {}; Skipping\n'
           sys.stderr.write(msg.format(rowID, row[f]))
           to_rm.append(row[config['samples_col']])
ssw('\33[33mNumber of skipped sample table entries: {}\n\x1b[0m'.format(len(to_rm)))
config['samples'].drop(to_rm, inplace=True)
if config['samples'].shape[0] < 1:
    raise ValueError('No genomes remaining after filtering!')
config['samples_unique'] = config['samples'][config['samples_col']].unique().tolist()

## temp_folder
config['pipeline']['username'] = getpass.getuser()
config['pipeline']['email'] = config['pipeline']['username'] + '@tuebingen.mpg.de'
config['tmp_dir'] = os.path.join(config['tmp_dir'], config['pipeline']['username'])
config['tmp_dir'] = os.path.join(config['tmp_dir'], 'Struo2_' + str(os.stat('.').st_ino) + '/')
print('\33[33mUsing temporary directory: {} \x1b[0m'.format(config['tmp_dir']))

## batches
config['params']['humann3']['splits'] = \
  make_fasta_splits(config['params']['humann3']['batches'])

## including modular snakefiles
print('\33[36m--Running db-create pipeline--\x1b[0m')
snake_dir = config['pipeline']['snakemake_folder']
include: snake_dir + 'bin/dirs'
### kraken/bracken
if not skipped(config['databases']['kraken2']):
    print('\33[36m* Creating kraken2 database\x1b[0m')
    include: snake_dir + 'bin/db_create/kraken2/Snakefile'
    if not skipped(config['databases']['bracken']):
        print('\33[36m* Creating bracken database\x1b[0m')
        include: snake_dir + 'bin/db_create/bracken/Snakefile'
### genes
if not skipped(config['databases']['genes']):
    print('\33[36m* Creating genes database\x1b[0m')
    include: snake_dir + 'bin/db_create/genes/Snakefile'
else:
    m = '\33[33m* Skipping creation of genes database;'
    m += ' assuming the database already exist!\x1b[0m'
    print(m)
### humann3
if not skipped(config['databases']['humann3_bowtie2']) and \
   not skipped(config['databases']['humann3_diamond']):
    print('\33[36m* Creating humann3 database\x1b[0m')
    include: snake_dir + 'bin/db_create/humann3/Snakefile'
if not skipped(config['databases']['metaphlan3']):
    include: snake_dir + 'bin/db_create/metaphlan3/Snakefile'

# final output files
def all_which_input(wildcards):
    """
    The final output files for both db_create & db_update
    """
    input_files = []    
    # kraken2
    if not skipped(config['databases']['kraken2']):
        input_files.append(os.path.join(kraken2_dir, 'hash.k2d'))
        input_files.append(os.path.join(kraken2_dir, 'opts.k2d'))
        input_files.append(os.path.join(kraken2_dir, 'taxo.k2d'))
        input_files.append(os.path.join(kraken2_dir, 'seqid2taxid.map'))
        # bracken
        if not skipped(config['databases']['bracken']):
    	    x = expand(os.path.join(kraken2_dir, 'database{read_len}mers.kraken'),
	               read_len = config['params']['bracken']['build_read_lens'])
            input_files += x
    # genes
    if not skipped(config['databases']['genes']):
        input_files.append(genes_dir + 'genome_reps_filtered.fna.gz')
        input_files.append(genes_dir + 'genome_reps_filtered.faa.gz')
        input_files.append(genes_dir + 'genome_reps_filtered.txt.gz')
        if str(config['keep_intermediate']) == 'True':
            # mmseqs gene database
            input_files.append(genes_dir + 'genes_db.tar.gz')
            ## mmseqs cluster database
            input_files.append(genes_dir + 'cluster/clusters_db.tar.gz')
            ## cluster membership info
            input_files.append(genes_dir + 'cluster/clusters_membership.tsv.gz')
            ## cluster rep sequences
            input_files.append(genes_dir + 'cluster/clusters_reps.faa.gz')                                    
    # humann3
    if (not skipped(config['databases']['humann3_bowtie2']) and
        not skipped(config['databases']['humann3_diamond'])):
        # annotation hits
        input_files.append(humann3_dir + 'annotation_hits.gz')
        ## annotated genes (all)
        input_files.append(humann3_dir + 'genome_reps_filt_annot.fna.gz')
        input_files.append(humann3_dir + 'genome_reps_filt_annot.faa.gz')
        input_files.append(humann3_dir + 'genome_reps_filt_annot.tsv.gz')
        ## databases
        if not skipped(config['databases']['humann3_bowtie2']):
            input_files.append(os.path.join(humann3_dir, 'bowtie2_build.done'))
        if not skipped(config['databases']['humann3_diamond']):
            input_files.append(os.path.join(humann3_dir, 'protein_database',
                                            config['dmnd_name']))
    
    # ret
    return input_files
